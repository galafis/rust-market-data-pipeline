# ğŸ”„ Rust Market Data Pipeline

![Rust](https://img.shields.io/badge/rust-%23000000.svg?style=for-the-badge&logo=rust&logoColor=white)
![License](https://img.shields.io/badge/license-MIT-blue.svg?style=for-the-badge)
![Tokio](https://img.shields.io/badge/tokio-async-green.svg?style=for-the-badge)
![Polars](https://img.shields.io/badge/polars-data-orange.svg?style=for-the-badge)
![Build](https://img.shields.io/badge/build-passing-brightgreen.svg?style=for-the-badge)

<div align="center">
  <img src="docs/images/hero.jpg" alt="Market Data Pipeline" width="800"/>
</div>

<div align="center">
  <h3>âš¡ High-performance data pipeline for financial market data</h3>
  <p>Ingest, process, and store market data with blazing speed and reliability</p>
</div>

---

## ğŸ‡§ğŸ‡· DescriÃ§Ã£o em PortuguÃªs

`rust-market-data-pipeline` Ã© um sistema robusto e escalÃ¡vel para **ingestÃ£o**, **processamento** e **armazenamento** de dados do mercado financeiro. ConstruÃ­do em Rust com Tokio para operaÃ§Ãµes assÃ­ncronas, oferece **baixa latÃªncia**, **alta throughput** e **confiabilidade** para aplicaÃ§Ãµes financeiras crÃ­ticas.

### âœ¨ Funcionalidades Principais

- ğŸ”Œ **Conectores de Dados** - IntegraÃ§Ã£o com APIs de mercado (Alpha Vantage, etc.)
- ğŸ”„ **Processamento AssÃ­ncrono** - Pipeline nÃ£o-bloqueante com Tokio
- ğŸ’¾ **Armazenamento Eficiente** - Formato colunar Apache Parquet
- ğŸ§¹ **Limpeza de Dados** - NormalizaÃ§Ã£o e transformaÃ§Ã£o automÃ¡tica
- ğŸ—ï¸ **Arquitetura Modular** - Crates independentes e reutilizÃ¡veis

---

## ğŸ‡ºğŸ‡¸ English Description

`rust-market-data-pipeline` is a robust and scalable system for **ingesting**, **processing**, and **storing** financial market data. Built in Rust with Tokio for asynchronous operations, it offers **low latency**, **high throughput**, and **reliability** for critical financial applications.

### âœ¨ Key Features

- ğŸ”Œ **Data Connectors** - Integration with market APIs (Alpha Vantage, etc.)
- ğŸ”„ **Asynchronous Processing** - Non-blocking pipeline with Tokio
- ğŸ’¾ **Efficient Storage** - Apache Parquet columnar format
- ğŸ§¹ **Data Cleaning** - Automatic normalization and transformation
- ğŸ—ï¸ **Modular Architecture** - Independent and reusable crates

---

## ğŸš€ Quick Start

### Prerequisites

- [Rust](https://www.rust-lang.org/tools/install) 1.70+
- Git
- [Alpha Vantage API Key](https://www.alphavantage.co/support/#api-key) (free)

### Installation

```bash
# Clone the repository
git clone https://github.com/galafis/rust-market-data-pipeline.git
cd rust-market-data-pipeline

# Run the example
cargo run --example data_pipeline
```

### Example Output

```
Dados brutos recebidos:
shape: (100, 6)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ date       â”† open   â”† high   â”† low     â”† close  â”† volume  â”‚
â”‚ ---        â”† ---    â”† ---    â”† ---     â”† ---    â”† ---     â”‚
â”‚ str        â”† f64    â”† f64    â”† f64     â”† f64    â”† i64     â”‚
â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•¡
â”‚ 2025-10-02 â”† 285.79 â”† 288.54 â”† 282.79  â”† 286.72 â”† 3814232 â”‚
â”‚ 2025-07-03 â”† 287.94 â”† 292.32 â”† 287.9   â”† 291.97 â”† 1853289 â”‚
â”‚ ...        â”† ...    â”† ...    â”† ...     â”† ...    â”† ...     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Dados processados e armazenados em output/ibm_daily.parquet
```

---

## ğŸ“š Usage Example

### Building a Custom Data Pipeline

```rust
use rmdp_data::fetch_market_data;
use rmdp_core::process_data;
use polars::prelude::*;

#[tokio::main]
async fn main() -> Result<()> {
    // Fetch data from API
    let raw_data = fetch_market_data(
        "IBM",
        "TIME_SERIES_DAILY",
        "YOUR_API_KEY"
    ).await?;

    // Process and clean data
    let processed_data = process_data(&raw_data)?;

    // Store in Parquet format
    ParquetWriter::new(File::create("output/data.parquet")?)
        .finish(&mut processed_data)?;

    println!("Pipeline completed successfully!");
    Ok(())
}
```

---

## ğŸ—ï¸ Architecture

The pipeline follows a modular ETL (Extract, Transform, Load) architecture:

<div align="center">
  <img src="docs/dataflow.png" alt="Data Flow Diagram" width="700"/>
</div>

### ETL Process

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Extract   â”‚ â”€â”€â”€> â”‚  Transform   â”‚ â”€â”€â”€> â”‚    Load     â”‚
â”‚  (Ingest)   â”‚      â”‚  (Process)   â”‚      â”‚  (Storage)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                     â”‚                      â”‚
   API Data          Clean & Normalize        Parquet
```

### Project Structure

```
rust-market-data-pipeline/
â”œâ”€â”€ crates/
â”‚   â”œâ”€â”€ core/          # Pipeline orchestration & processing
â”‚   â”œâ”€â”€ data/          # Data connectors & API integrations
â”‚   â””â”€â”€ utils/         # Logging & utilities
â”œâ”€â”€ examples/          # Usage examples
â”œâ”€â”€ output/           # Processed data storage
â””â”€â”€ docs/             # Documentation & images
```

### Crate Descriptions

| Crate | Description |
|-------|-------------|
| **rmdp-core** | Pipeline orchestration and data processing logic |
| **rmdp-data** | API connectors for external data sources |
| **rmdp-utils** | Logging and utility functions |

---

## ğŸ“Š Supported Data Sources

- âœ… **Alpha Vantage** - Stock market data
- ğŸ”œ **Yahoo Finance** - Historical and real-time quotes
- ğŸ”œ **CoinGecko** - Cryptocurrency data
- ğŸ”œ **Binance** - Crypto exchange data
- ğŸ”œ **IEX Cloud** - Financial data API

---

## ğŸ›£ï¸ Roadmap

- [ ] Add support for more data sources (Yahoo Finance, CoinGecko)
- [ ] Implement real-time streaming with WebSockets
- [ ] Add Redis caching layer for API responses
- [ ] Integration with time-series databases (InfluxDB, TimescaleDB)
- [ ] Create monitoring dashboard for pipeline health
- [ ] Add data quality validation and alerts
- [ ] Implement retry logic and error handling
- [ ] Support for multiple output formats (CSV, JSON, Parquet)

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/DataSource`)
3. Commit your changes (`git commit -m 'Add new data source'`)
4. Push to the branch (`git push origin feature/DataSource`)
5. Open a Pull Request

---

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¨â€ğŸ’» Author

**Gabriel Demetrios Lafis**

- ğŸ“ Systems Analysis and Development | IT Management | Cybersecurity
- ğŸ’¼ Data Scientist | Data Analyst | BI/BA
- ğŸ”— [GitHub](https://github.com/galafis)

---

## ğŸ™ Acknowledgments

- Built with [Rust](https://www.rust-lang.org/)
- Async runtime: [Tokio](https://tokio.rs/)
- Data processing: [Polars](https://www.pola.rs/)
- Storage format: [Apache Parquet](https://parquet.apache.org/)

---

<div align="center">
  <p>Made with â¤ï¸ and Rust</p>
  <p>â­ Star this repository if you find it useful!</p>
</div>
